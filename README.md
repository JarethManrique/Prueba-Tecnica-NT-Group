# Prueba TÃ©cnica â€“ NT Group

Este proyecto contiene una soluciÃ³n dividida en dos partes: modelado y carga de datos en PostgreSQL (SecciÃ³n 1), y desarrollo de una API REST en Python (SecciÃ³n 2).

---

## ğŸ§© SecciÃ³n 1: Modelado, carga y exportaciÃ³n de datos

### 1.1 ElecciÃ³n de la base de datos

ElegÃ­ usar PostgreSQL como sistema de base de datos relacional porque ofrece una buena combinaciÃ³n de caracterÃ­sticas que se ajustan a este tipo de ejercicio.
Me pareciÃ³ una buena opciÃ³n por su capacidad para manejar datos con integridad (gracias al soporte de llaves primarias y validaciones), la flexibilidad en el manejo de distintos tipos de datos, y su buena integraciÃ³n con Python (especialmente usando psycopg2).
TambiÃ©n es compatible con Docker, lo que facilita mucho el despliegue, y permite construir consultas SQL complejas, como las que se requieren en esta prueba.

### 1.2 ExportaciÃ³n de datos

ElegÃ­ usar Python para la extracciÃ³n de datos porque ya estaba trabajando con Ã©l en el resto del proceso, y tiene buenas herramientas para conectarse a bases de datos y trabajar con estructuras tabulares.
Para el formato de salida, seleccionÃ© CSV porque es ampliamente compatible, fÃ¡cil de leer, portable y suficiente para representar los datos extraÃ­dos en este caso. AdemÃ¡s, pandas permite exportar a este formato con una sola lÃ­nea de cÃ³digo.

### 1.3 Transformaciones aplicadas

Para poder insertar los datos en la tabla charges, tuve que hacer varias transformaciones desde Python.
ConvertÃ­ las fechas al formato correcto (created_at y paid_at), validÃ© que los valores en amount fueran numÃ©ricos y que no se pasaran del lÃ­mite que acepta PostgreSQL.
TambiÃ©n eliminÃ© filas sin id porque esa columna es clave primaria, y reemplacÃ© valores vacÃ­os por None para que se insertaran como NULL.
Lo mÃ¡s complicado fue manejar datos mal formateados o vacÃ­os, especialmente en las fechas y montos, ya que causaban errores si no los filtraba antes.

### 1.4 Diagrama de base de datos

![Diagrama](Diagram_1.4.png)

### 1.5 Resumen final

Se utilizÃ³ PostgreSQL para almacenar y modelar los datos por su robustez e integraciÃ³n con Python y Docker.  
Se cargaron y transformaron los datos desde un `.csv`, limpiando fechas, montos y valores nulos.  
Se separÃ³ la informaciÃ³n de compaÃ±Ã­as en la tabla `companies`, relacionada con `charges` por `company_id`.  
Se creÃ³ la vista `daily_company_totals` para consultar el total transaccionado por dÃ­a y compaÃ±Ã­a.  
Se incluyÃ³ el script `query_view.py` para revisar fÃ¡cilmente los datos de la vista desde Python.

---

## ğŸš€ SecciÃ³n 2: API REST

La API fue desarrollada con **FastAPI** y contiene dos endpoints principales:

- `POST /extract` â†’ Recibe un nÃºmero y lo guarda si no ha sido extraÃ­do aÃºn.
- `GET /find-missing` â†’ Devuelve el nÃºmero faltante si solo hay uno.

Se utilizÃ³ una clase que simula el conjunto de nÃºmeros del 1 al 100 y gestiona los extraÃ­dos. La API valida entradas, muestra errores claros y fue dockerizada para facilitar su ejecuciÃ³n.

---

## â–¶ï¸ EjecuciÃ³n local

1. Instalar dependencias:

```
pip install -r requirements.txt
```

2. Ejecutar la API:

```
uvicorn main:app --reload
```

3. Ir a [http://localhost:8000/docs](http://localhost:8000/docs) para probar los endpoints.

---

## ğŸ³ EjecuciÃ³n con Docker

1. Construir la imagen:

```
docker build -t fastapi-numbers .
```

2. Ejecutar el contenedor:

```
docker run -d -p 8000:8000 fastapi-numbers
```

3. Probar en [http://localhost:8000/docs](http://localhost:8000/docs)

---

## âœ… Estructura de archivos

```
Seccion 1/
â”œâ”€â”€ db_connection.py
â”œâ”€â”€ insert_data.py
â”œâ”€â”€ extract_data.py
â”œâ”€â”€ table.py
â”œâ”€â”€ query_view.py
â”œâ”€â”€ main.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ charges_export.csv
â”œâ”€â”€ data_prueba_tecnica.csv

Seccion 2/
â”œâ”€â”€ main.py
â”œâ”€â”€ model.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
```

---

Desarrollado por: **Jareth Manrique**
